{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "       load parameters from pre-train models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        '''\n",
    "        \n",
    "        print('building model finished: '+str(time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = '/home/wzw/Downloads/vgg16.npy'\n",
    "content_img_path = '/home/wzw/Downloads/content^.jpeg'\n",
    "style_img_path = '/home/wzw/Downloads/style^.jpeg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = '/home/wzw/tf-learning/style_transfer/run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model ...\n",
      "building model finished: 0.220014095306\n",
      "building model ...\n",
      "building model finished: 0.202681064606\n",
      "building model ...\n",
      "building model finished: 0.227824926376\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\n",
    "    Args:\n",
    "    - x: feaures extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path).item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    # vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_gram = \\\n",
    "    [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1, 2], [3, 4], zip([1,2], [3,4]) -> [(1, 3), (2, 4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 15871.8037, content_loss: 62639.8711, style_loss:  19.2156\n",
      "step: 2, loss_value: 13938.1055, content_loss: 53283.4883, style_loss:  17.2195\n",
      "step: 3, loss_value: 11193.3975, content_loss: 47572.9961, style_loss:  12.8722\n",
      "step: 4, loss_value: 9309.2910, content_loss: 44075.2344, style_loss:   9.8035\n",
      "step: 5, loss_value: 8990.6152, content_loss: 42116.8711, style_loss:   9.5579\n",
      "step: 6, loss_value: 7943.3389, content_loss: 40952.1211, style_loss:   7.6963\n",
      "step: 7, loss_value: 7302.8447, content_loss: 40297.3672, style_loss:   6.5462\n",
      "step: 8, loss_value: 6685.4458, content_loss: 40088.5312, style_loss:   5.3532\n",
      "step: 9, loss_value: 6371.8750, content_loss: 40042.1328, style_loss:   4.7353\n",
      "step: 10, loss_value: 6042.1182, content_loss: 39990.7891, style_loss:   4.0861\n",
      "step: 11, loss_value: 5861.9038, content_loss: 39931.8203, style_loss:   3.7374\n",
      "step: 12, loss_value: 5696.9121, content_loss: 39818.0469, style_loss:   3.4302\n",
      "step: 13, loss_value: 5526.9375, content_loss: 39489.5391, style_loss:   3.1560\n",
      "step: 14, loss_value: 5402.7041, content_loss: 38991.9727, style_loss:   3.0070\n",
      "step: 15, loss_value: 5238.3599, content_loss: 38410.4336, style_loss:   2.7946\n",
      "step: 16, loss_value: 5128.9365, content_loss: 37726.2461, style_loss:   2.7126\n",
      "step: 17, loss_value: 4951.0391, content_loss: 36855.9531, style_loss:   2.5309\n",
      "step: 18, loss_value: 4828.6982, content_loss: 35886.4531, style_loss:   2.4801\n",
      "step: 19, loss_value: 4654.3115, content_loss: 34899.1797, style_loss:   2.3288\n",
      "step: 20, loss_value: 4531.3779, content_loss: 33861.3555, style_loss:   2.2905\n",
      "step: 21, loss_value: 4372.9941, content_loss: 32757.9492, style_loss:   2.1944\n",
      "step: 22, loss_value: 4232.6006, content_loss: 31676.0664, style_loss:   2.1300\n",
      "step: 23, loss_value: 4101.6240, content_loss: 30623.8652, style_loss:   2.0785\n",
      "step: 24, loss_value: 3963.5774, content_loss: 29556.3652, style_loss:   2.0159\n",
      "step: 25, loss_value: 3845.6528, content_loss: 28525.0781, style_loss:   1.9863\n",
      "step: 26, loss_value: 3730.8284, content_loss: 27571.7852, style_loss:   1.9473\n",
      "step: 27, loss_value: 3614.8259, content_loss: 26659.4316, style_loss:   1.8978\n",
      "step: 28, loss_value: 3514.3738, content_loss: 25813.5391, style_loss:   1.8660\n",
      "step: 29, loss_value: 3416.3826, content_loss: 25035.6445, style_loss:   1.8256\n",
      "step: 30, loss_value: 3329.5413, content_loss: 24285.3184, style_loss:   1.8020\n",
      "step: 31, loss_value: 3241.8047, content_loss: 23617.9355, style_loss:   1.7600\n",
      "step: 32, loss_value: 3161.3745, content_loss: 22986.4434, style_loss:   1.7255\n",
      "step: 33, loss_value: 3088.5798, content_loss: 22402.8164, style_loss:   1.6966\n",
      "step: 34, loss_value: 3021.7170, content_loss: 21862.6562, style_loss:   1.6709\n",
      "step: 35, loss_value: 2963.2754, content_loss: 21337.6445, style_loss:   1.6590\n",
      "step: 36, loss_value: 2914.3638, content_loss: 20882.7559, style_loss:   1.6522\n",
      "step: 37, loss_value: 2907.4761, content_loss: 20399.0586, style_loss:   1.7351\n",
      "step: 38, loss_value: 2865.5488, content_loss: 20049.7500, style_loss:   1.7211\n",
      "step: 39, loss_value: 2825.2024, content_loss: 19611.3594, style_loss:   1.7281\n",
      "step: 40, loss_value: 2695.3936, content_loss: 19285.3164, style_loss:   1.5337\n",
      "step: 41, loss_value: 2683.8145, content_loss: 18962.1094, style_loss:   1.5752\n",
      "step: 42, loss_value: 2690.3123, content_loss: 18589.7266, style_loss:   1.6627\n",
      "step: 43, loss_value: 2571.6426, content_loss: 18317.9238, style_loss:   1.4797\n",
      "step: 44, loss_value: 2552.7109, content_loss: 18027.7090, style_loss:   1.4999\n",
      "step: 45, loss_value: 2564.4446, content_loss: 17703.3066, style_loss:   1.5882\n",
      "step: 46, loss_value: 2475.2322, content_loss: 17474.4648, style_loss:   1.4556\n",
      "step: 47, loss_value: 2427.9106, content_loss: 17196.7227, style_loss:   1.4165\n",
      "step: 48, loss_value: 2420.3320, content_loss: 16908.1406, style_loss:   1.4590\n",
      "step: 49, loss_value: 2381.1543, content_loss: 16690.1895, style_loss:   1.4243\n",
      "step: 50, loss_value: 2342.8691, content_loss: 16419.1855, style_loss:   1.4019\n",
      "step: 51, loss_value: 2295.9507, content_loss: 16184.1836, style_loss:   1.3551\n",
      "step: 52, loss_value: 2275.7683, content_loss: 15956.9160, style_loss:   1.3602\n",
      "step: 53, loss_value: 2274.3640, content_loss: 15700.3594, style_loss:   1.4087\n",
      "step: 54, loss_value: 2238.0452, content_loss: 15515.9287, style_loss:   1.3729\n",
      "step: 55, loss_value: 2217.6245, content_loss: 15256.0293, style_loss:   1.3840\n",
      "step: 56, loss_value: 2184.1501, content_loss: 15079.6182, style_loss:   1.3524\n",
      "step: 57, loss_value: 2147.2065, content_loss: 14860.0215, style_loss:   1.3224\n",
      "step: 58, loss_value: 2109.7471, content_loss: 14672.9492, style_loss:   1.2849\n",
      "step: 59, loss_value: 2096.8335, content_loss: 14490.1865, style_loss:   1.2956\n",
      "step: 60, loss_value: 2083.4558, content_loss: 14293.6738, style_loss:   1.3082\n",
      "step: 61, loss_value: 2060.9819, content_loss: 14144.4072, style_loss:   1.2931\n",
      "step: 62, loss_value: 2082.3977, content_loss: 13937.6377, style_loss:   1.3773\n",
      "step: 63, loss_value: 2097.7002, content_loss: 13826.3213, style_loss:   1.4301\n",
      "step: 64, loss_value: 2140.6880, content_loss: 13609.2949, style_loss:   1.5595\n",
      "step: 65, loss_value: 2018.8137, content_loss: 13540.2041, style_loss:   1.3296\n",
      "step: 66, loss_value: 2041.1731, content_loss: 13425.8555, style_loss:   1.3972\n",
      "step: 67, loss_value: 2002.7410, content_loss: 13288.3164, style_loss:   1.3478\n",
      "step: 68, loss_value: 2006.5415, content_loss: 13219.8467, style_loss:   1.3691\n",
      "step: 69, loss_value: 1980.7025, content_loss: 13102.7764, style_loss:   1.3408\n",
      "step: 70, loss_value: 1933.5219, content_loss: 12994.4209, style_loss:   1.2682\n",
      "step: 71, loss_value: 1947.2629, content_loss: 12893.6152, style_loss:   1.3158\n",
      "step: 72, loss_value: 1900.6011, content_loss: 12754.4473, style_loss:   1.2503\n",
      "step: 73, loss_value: 1908.1948, content_loss: 12639.7656, style_loss:   1.2884\n",
      "step: 74, loss_value: 1864.2681, content_loss: 12486.1494, style_loss:   1.2313\n",
      "step: 75, loss_value: 1856.0541, content_loss: 12365.5664, style_loss:   1.2390\n",
      "step: 76, loss_value: 1834.0483, content_loss: 12223.4492, style_loss:   1.2234\n",
      "step: 77, loss_value: 1810.7256, content_loss: 12102.7139, style_loss:   1.2009\n",
      "step: 78, loss_value: 1808.9419, content_loss: 11951.2178, style_loss:   1.2276\n",
      "step: 79, loss_value: 1771.4241, content_loss: 11832.0947, style_loss:   1.1764\n",
      "step: 80, loss_value: 1773.4341, content_loss: 11695.7617, style_loss:   1.2077\n",
      "step: 81, loss_value: 1751.0332, content_loss: 11585.4609, style_loss:   1.1850\n",
      "step: 82, loss_value: 1746.0916, content_loss: 11430.9014, style_loss:   1.2060\n",
      "step: 83, loss_value: 1769.8252, content_loss: 11338.0537, style_loss:   1.2720\n",
      "step: 84, loss_value: 1862.6589, content_loss: 11170.0508, style_loss:   1.4913\n",
      "step: 85, loss_value: 1846.4116, content_loss: 11170.6611, style_loss:   1.4587\n",
      "step: 86, loss_value: 1843.0544, content_loss: 11029.1211, style_loss:   1.4803\n",
      "step: 87, loss_value: 1706.6633, content_loss: 11010.7461, style_loss:   1.2112\n",
      "step: 88, loss_value: 1798.8462, content_loss: 11014.4551, style_loss:   1.3948\n",
      "step: 89, loss_value: 1762.3828, content_loss: 10919.0742, style_loss:   1.3410\n",
      "step: 90, loss_value: 1725.1169, content_loss: 10889.6445, style_loss:   1.2723\n",
      "step: 91, loss_value: 1713.8201, content_loss: 10854.1113, style_loss:   1.2568\n",
      "step: 92, loss_value: 1688.9523, content_loss: 10769.9932, style_loss:   1.2239\n",
      "step: 93, loss_value: 1684.6184, content_loss: 10697.0703, style_loss:   1.2298\n",
      "step: 94, loss_value: 1651.4053, content_loss: 10615.3281, style_loss:   1.1797\n",
      "step: 95, loss_value: 1650.2562, content_loss: 10507.4580, style_loss:   1.1990\n",
      "step: 96, loss_value: 1618.0090, content_loss: 10406.1611, style_loss:   1.1548\n",
      "step: 97, loss_value: 1617.0527, content_loss: 10302.2412, style_loss:   1.1737\n",
      "step: 98, loss_value: 1594.4580, content_loss: 10183.9404, style_loss:   1.1521\n",
      "step: 99, loss_value: 1584.9072, content_loss: 10073.2422, style_loss:   1.1552\n",
      "step: 100, loss_value: 1577.2434, content_loss: 9958.6758, style_loss:   1.1628\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content: content_val,\n",
    "                         style: style_val,\n",
    "                     })\n",
    "        print 'step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "            % (step+1,\n",
    "               loss_value[0],\n",
    "               content_loss_value[0],\n",
    "               style_loss_value[0])\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step+1))\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
